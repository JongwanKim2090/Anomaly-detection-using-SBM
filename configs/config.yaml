# @package _global_

# specify here default training configuration
defaults:
  - trainer: default.yaml
  - model: ddpm.yaml
  - datamodule: cifar10.yaml
  - experiment: null

  - hydra: default.yaml

  # enable color logging
  - override hydra/hydra_logging: colorlog
  - override hydra/job_logging: colorlog

work_dir: ${hydra:runtime.cwd}

# path to folder with data
data_dir: ${work_dir}/mvtec_anomaly_detection/

callbacks:
  model_checkpoint:
    _target_: pytorch_lightning.callbacks.ModelCheckpoint
    monitor: "valid_loss"
    save_top_k: 2
    save_last: True
    mode: "min"
    dirpath: "checkpoints/"
    filename: "sample-{epoch:02d}"
  early_stopping:
    _target_: pytorch_lightning.callbacks.EarlyStopping
    monitor: "valid_loss"
    patience: 100
    mode: "min"
    verbose: True
    check_finite: True

logger:
  _target_: pytorch_lightning.loggers.wandb.WandbLogger
  project: "Anomaly_detection_SBGM"
  save_dir: "."
  name: ${datamodule.class_name}_${datamodule.img_size}_${model.err_quantile}
  tags: [
      "${datamodule.name}", "${model.model_name}", "image_size:${datamodule.img_size}"
  ]

debug: False
print_config: True
ignore_warnings: True

enable_model_summary: True
test_after_training: False